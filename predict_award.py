"""
ç»Ÿä¸€å¥–é¡¹æ¦‚ç‡é¢„æµ‹è„šæœ¬ (v2)

è¾“å…¥: PDF æ–‡ä»¶è·¯å¾„
è¾“å‡º: å„å¥–é¡¹ (O/F/M/H/S) çš„æ¦‚ç‡åˆ†å¸ƒ

æ ¸å¿ƒæ”¹è¿›:
1. è‡ªåŠ¨è¯†åˆ«é¢˜ç›® (A-F) å’Œå¹´ä»½
2. ä½¿ç”¨é¢˜ç›®ç‰¹å®šçš„è·å¥–æ¯”ä¾‹å…ˆéªŒ
3. å¤šç»´è´¨é‡ä¿¡å·èåˆ (ç›¸ä¼¼åº¦ + ç»“æ„ + é¢˜ç›®é€‚é… + ç«äº‰å¼ºåº¦)
4. æ— éœ€ H/S/F æ ·æœ¬ï¼Œé€šè¿‡è´å¶æ–¯æ¨æ–­å®ç°å…¨å¥–é¡¹æ¦‚ç‡ä¼°è®¡

ä½¿ç”¨æ–¹æ³•:
    python predict_award.py <pdfæ–‡ä»¶è·¯å¾„> [--problem A] [--year 2024]
"""

import os
import sys
import pickle
import numpy as np
from pathlib import Path
import argparse
import logging
from sklearn.metrics.pairwise import cosine_similarity

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.pdf_parser import PDFParser
from src.text_features import TextFeatureExtractor
from src.image_features import ImageFeatureExtractor
from src.feature_fusion import fuse_features
from src.probability_model_v2 import EnhancedAwardEstimator
from src.problem_detector import ProblemDetector, detect_problem
from src.award_prior import get_award_prior, get_problem_profile, list_available_data

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


class AwardPredictor:
    """
    MCM/ICM è®ºæ–‡è·å¥–æ¦‚ç‡é¢„æµ‹å™¨ (v2)
    
    è¾“å…¥ PDF â†’ è¾“å‡º O/F/M/H/S å„å¥–é¡¹æ¦‚ç‡
    
    æ ¸å¿ƒæµç¨‹:
    1. è§£æ PDF â†’ æå–æ‘˜è¦ã€å›¾ç‰‡ã€å…¨æ–‡ã€ç»“æ„ä¿¡æ¯
    2. è‡ªåŠ¨è¯†åˆ«é¢˜ç›®ç±»å‹ (A-F) å’Œå¹´ä»½
    3. æå–ç‰¹å¾å‘é‡ (æ–‡æœ¬è¯­ä¹‰ + å›¾åƒ + å…ƒæ•°æ®)
    4. è®¡ç®—ä¸ O å¥–è´¨å¿ƒçš„ç›¸ä¼¼åº¦
    5. åŸºäºè´å¶æ–¯æ¨æ–­ä¼°è®¡å„å¥–é¡¹æ¦‚ç‡
    6. ç”¨ç»“æ„è´¨é‡ã€é¢˜ç›®é€‚é…åº¦ç­‰ä¿¡å·ä¿®æ­£æ¦‚ç‡
    """
    
    def __init__(self, model_path: str = "models/scoring_model.pkl"):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        å‚æ•°:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
        """
        logger.info("åˆå§‹åŒ– MCM/ICM è·å¥–æ¦‚ç‡é¢„æµ‹å™¨ v2...")
        
        # åŠ è½½æ¨¡å‹
        self.model_data = self._load_model(model_path)
        if self.model_data is None:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.pdf_parser = PDFParser()
        self.text_extractor = TextFeatureExtractor()
        self.image_extractor = ImageFeatureExtractor()
        self.problem_detector = ProblemDetector()
        
        # åˆå§‹åŒ–æ¦‚ç‡ä¼°è®¡å™¨
        prob_params = self.model_data.get('prob_model_params')
        if prob_params and 'award_distributions' in prob_params:
            self.prob_estimator = EnhancedAwardEstimator()
            self.prob_estimator.load_parameters(prob_params)
        else:
            # ä½¿ç”¨è®­ç»ƒé›†çš„ç›¸ä¼¼åº¦é‡æ–°æ‹Ÿåˆ
            stats = self.model_data['stats']
            # ä»statsä¸­æ¨¡æ‹ŸOå¥–ç›¸ä¼¼åº¦åˆ†å¸ƒ
            sim_mean = stats['similarity_mean']
            sim_std = stats['similarity_std']
            fake_sims = np.random.normal(sim_mean, sim_std, 50)
            self.prob_estimator = EnhancedAwardEstimator(fake_sims, stats)
        
        n_papers = self.model_data.get('n_papers', '?')
        logger.info(f"é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼ˆåŸºäº {n_papers} ç¯‡ O å¥–è®ºæ–‡è®­ç»ƒï¼‰\n")
    
    def _load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_path):
            logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            logger.error("è¯·å…ˆè¿è¡Œ: python scripts/train_scoring_model.py")
            return None
        
        with open(model_path, 'rb') as f:
            return pickle.load(f)
    
    def predict(self, pdf_path: str, problem: str = None, 
                year: int = None, verbose: bool = True) -> dict:
        """
        é¢„æµ‹å•ç¯‡è®ºæ–‡çš„å„å¥–é¡¹æ¦‚ç‡
        
        å‚æ•°:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            problem: é¢˜ç›® (A-F)ï¼ŒNone åˆ™è‡ªåŠ¨æ£€æµ‹
            year: å¹´ä»½ï¼ŒNone åˆ™è‡ªåŠ¨æ£€æµ‹
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        è¿”å›:
            {
                'probabilities': {'O': 0.01, 'F': 0.03, 'M': 0.45, ...},
                'score': 78.5,
                'aspect_scores': {'abstract': 72, 'figures': 65, 'modeling': 80},
                'aspect_details': {...},
                'similarity': 0.823,
                'problem': 'A',
                'contest': 'MCM',
                'year': 2024,
                'quality_tier': 'è‰¯å¥½æ°´å¹³',
                'description': '...',
                'metadata': {...},
                'success': True,
            }
        """
        if not os.path.exists(pdf_path):
            return {'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}', 'success': False}
        
        try:
            # ==================== æ­¥éª¤1: è§£æ PDF ====================
            if verbose:
                logger.info(f"ğŸ“„ è®ºæ–‡: {os.path.basename(pdf_path)}")
                logger.info("=" * 60)
                logger.info("  [1/7] è§£æ PDF...")
            
            parsed = self.pdf_parser.parse(pdf_path)
            
            if not parsed.get('success', False):
                return {'error': 'PDF è§£æå¤±è´¥', 'success': False}
            
            abstract = parsed['abstract']
            full_text = parsed.get('full_text', '')
            images = parsed['images']
            structure = parsed.get('structure', {})
            metadata = parsed.get('metadata', {})
            
            if verbose:
                logger.info(f"    âœ“ æ‘˜è¦: {len(abstract)} å­—ç¬¦")
                logger.info(f"    âœ“ å…¨æ–‡: {len(full_text)} å­—ç¬¦")
                logger.info(f"    âœ“ å›¾ç‰‡: {len(images)} å¼ ")
                logger.info(f"    âœ“ é¡µæ•°: {metadata.get('page_count', '?')} é¡µ")
            
            # ==================== æ­¥éª¤2: è‡ªåŠ¨è¯†åˆ«é¢˜ç›® ====================
            if verbose:
                logger.info("  [2/7] è‡ªåŠ¨è¯†åˆ«é¢˜ç›®...")
            
            if not problem or not year:
                detection = self.problem_detector.detect(full_text, pdf_path, year)
                
                if not problem:
                    problem = detection.get('problem', 'A')
                if not year:
                    year = detection.get('year') or self.problem_detector.detect_year(full_text, pdf_path) or 2024
                
                contest = detection.get('contest', 'MCM' if problem in 'ABC' else 'ICM')
                confidence = detection.get('confidence', 0.5)
                
                if verbose:
                    logger.info(f"    âœ“ é¢˜ç›®: Problem {problem} ({contest})")
                    logger.info(f"    âœ“ å¹´ä»½: {year}")
                    logger.info(f"    âœ“ æ£€æµ‹ç½®ä¿¡åº¦: {confidence:.0%}")
            else:
                contest = 'MCM' if problem.upper() in 'ABC' else 'ICM'
            
            problem = problem.upper()
            
            # ==================== æ­¥éª¤3: æå–ç‰¹å¾ ====================
            if verbose:
                logger.info("  [3/7] æå–æ–‡æœ¬ç‰¹å¾...")
            
            text_result = self.text_extractor.extract(
                abstract, full_text=full_text, structure=structure
            )
            text_feat = text_result['feature_vector']
            
            if verbose:
                logger.info("  [4/7] æå–å›¾åƒç‰¹å¾...")
            
            image_result = self.image_extractor.extract(images)
            image_feat = image_result['feature_vector']
            
            # ==================== æ­¥éª¤4: èåˆç‰¹å¾ & è®¡ç®—ç›¸ä¼¼åº¦ ====================
            if verbose:
                logger.info("  [5/7] ç‰¹å¾èåˆ...")
            
            page_count = metadata.get('page_count', 20)
            ref_count = metadata.get('ref_count', 15)
            
            fused = fuse_features(
                text_features=text_feat,
                image_features=image_feat,
                year=year,
                contest=contest,
                problem=problem,
                page_count=page_count,
                ref_count=ref_count,
            )
            
            # è®¡ç®—ä¸ O å¥–è´¨å¿ƒçš„ä½™å¼¦ç›¸ä¼¼åº¦
            centroid = self.model_data['centroid']
            stats = self.model_data['stats']
            
            fused, centroid = self._align_dimensions(fused, centroid)
            
            similarity = cosine_similarity(
                fused.reshape(1, -1), centroid.reshape(1, -1)
            )[0, 0]
            
            score = self._compute_score(similarity, stats)
            
            # ==================== æ­¥éª¤5: ä¸‰ç»´åº¦å­åˆ† ====================
            if verbose:
                logger.info("  [6/7] ä¸‰ç»´åº¦å¯¹æ¯”è¯„åˆ†...")
            
            aspect_scores, aspect_details = self._compute_aspect_scores(
                text_result, image_result, structure, images,
                abstract, full_text, page_count, ref_count,
            )
            
            # ==================== æ­¥éª¤6: è´å¶æ–¯æ¦‚ç‡ä¼°è®¡ ====================
            if verbose:
                logger.info("  [7/7] è´å¶æ–¯æ¦‚ç‡ä¼°è®¡...")
            
            probabilities = self.prob_estimator.estimate_probabilities(
                similarity=similarity,
                problem=problem,
                year=year,
                score=score,
                structure_info=structure,
                full_text=full_text,
                aspect_scores=aspect_scores,
            )
            
            quality_tier, emoji = EnhancedAwardEstimator.compute_quality_tier(probabilities)
            description = EnhancedAwardEstimator.get_award_description(probabilities)
            
            result = {
                'probabilities': probabilities,
                'score': score,
                'aspect_scores': aspect_scores,
                'aspect_details': aspect_details,
                'similarity': float(similarity),
                'problem': problem,
                'contest': contest,
                'year': year,
                'quality_tier': quality_tier,
                'emoji': emoji,
                'description': description,
                'metadata': {
                    'abstract_length': len(abstract),
                    'full_text_length': len(full_text),
                    'image_count': len(images),
                    'page_count': page_count,
                    'ref_count': ref_count,
                    'structure': structure,
                },
                'success': True,
            }
            
            if verbose:
                self.print_result(result)
            
            return result
            
        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {'error': str(e), 'success': False}
    
    def _align_dimensions(self, features: np.ndarray, centroid: np.ndarray):
        """è‡ªåŠ¨å¯¹é½ç‰¹å¾å‘é‡ä¸è´¨å¿ƒçš„ç»´åº¦"""
        f_dim = features.shape[0]
        c_dim = centroid.shape[0]
        if f_dim == c_dim:
            return features, centroid
        diff = abs(f_dim - c_dim)
        if diff > 50:
            logger.warning(f"âš ï¸ ç‰¹å¾ç»´åº¦å·®å¼‚è¿‡å¤§ ({f_dim} vs {c_dim})ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹")
        min_dim = min(f_dim, c_dim)
        return features[:min_dim], centroid[:min_dim]
    
    # ------------------------------------------------------------------
    #  ä¸‰ç»´åº¦å¯¹æ¯”è¯„åˆ† (æ‘˜è¦ / å›¾è¡¨ / å»ºæ¨¡)
    # ------------------------------------------------------------------
    def _compute_aspect_scores(
        self, text_result, image_result, structure, images,
        abstract, full_text, page_count, ref_count,
    ):
        """
        è®¡ç®—ä¸‰ä¸ªç»´åº¦çš„å­åˆ† (0-100)ï¼Œå¹¶ç”Ÿæˆä¸ O å¥–è®ºæ–‡çš„æ–‡å­—å·®å¼‚è¯´æ˜ã€‚

        ç»´åº¦:
          1. æ‘˜è¦å†™ä½œ (abstract)  â€” è¯­ä¹‰åµŒå…¥ç›¸ä¼¼åº¦ + å†™ä½œç»Ÿè®¡
          2. å›¾è¡¨æ°´å¹³ (figures)   â€” å›¾åƒç‰¹å¾ç›¸ä¼¼åº¦ + æ•°é‡/è´¨é‡
          3. å»ºæ¨¡æ·±åº¦ (modeling)  â€” ç»“æ„ç‰¹å¾ç›¸ä¼¼åº¦ + å…¬å¼/å¼•ç”¨/çµæ•åº¦

        è¿”å›:
          aspect_scores: {'abstract': 72, 'figures': 65, 'modeling': 80}
          aspect_details: {'abstract': {...detail dict...}, ...}
        """
        aspect_stats = self.model_data.get('aspect_stats', {})
        scores = {}
        details = {}

        # ---------- 1. æ‘˜è¦ç»´åº¦ ----------
        abs_stat = aspect_stats.get('abstract', {})
        abs_centroid = abs_stat.get('centroid')
        sem_feat = text_result.get('semantic_features')

        if abs_centroid is not None and sem_feat is not None:
            abs_centroid_arr = np.array(abs_centroid)
            min_d = min(len(sem_feat), len(abs_centroid_arr))
            sim = float(cosine_similarity(
                sem_feat[:min_d].reshape(1, -1),
                abs_centroid_arr[:min_d].reshape(1, -1)
            )[0, 0])
            abs_score = self._sim_to_score(
                sim, abs_stat.get('sim_mean', 0.7), abs_stat.get('sim_std', 0.1),
                abs_stat.get('sim_min', 0.3), abs_stat.get('sim_max', 1.0),
            )
        else:
            abs_score = 60.0
            sim = 0.0

        # ç»Ÿè®¡å¯¹æ¯”
        o_avg_len = abs_stat.get('avg_length', 1200)
        abs_len = len(abstract)
        len_ratio = abs_len / max(o_avg_len, 1)

        abs_diff = []
        if len_ratio < 0.5:
            abs_diff.append(f"æ‘˜è¦é•¿åº¦åçŸ­ ({abs_len}å­—ç¬¦ vs Oå¥–å¹³å‡{o_avg_len:.0f}å­—ç¬¦)")
            abs_score *= 0.9
        elif len_ratio > 2.0:
            abs_diff.append(f"æ‘˜è¦åé•¿ ({abs_len}å­—ç¬¦ vs Oå¥–å¹³å‡{o_avg_len:.0f}å­—ç¬¦)")
        else:
            abs_diff.append(f"æ‘˜è¦é•¿åº¦åˆç† ({abs_len}å­—ç¬¦, Oå¥–å¹³å‡{o_avg_len:.0f}å­—ç¬¦)")

        stat_feat = text_result.get('statistical_features', np.zeros(6))
        if stat_feat[3] < 0.5:
            abs_diff.append("æ‘˜è¦ç¼ºå°‘å®šé‡ç»“æœ(æ•°å­—/ç™¾åˆ†æ¯”)ï¼ŒOå¥–è®ºæ–‡æ‘˜è¦é€šå¸¸åŒ…å«å…³é”®æ•°æ®")
        if stat_feat[4] < 0.15:
            abs_diff.append("æŠ€æœ¯æœ¯è¯­å¯†åº¦åä½ï¼Œå»ºè®®ä½¿ç”¨æ›´ä¸“ä¸šçš„å­¦æœ¯è¡¨è¾¾")

        abs_score = float(np.clip(abs_score, 0, 100))
        scores['abstract'] = round(abs_score, 1)
        details['abstract'] = {
            'score': round(abs_score, 1),
            'similarity': round(sim, 4),
            'differences': abs_diff,
        }

        # ---------- 2. å›¾è¡¨ç»´åº¦ ----------
        fig_stat = aspect_stats.get('figures', {})
        fig_centroid = fig_stat.get('centroid')
        img_feat = image_result.get('feature_vector')

        if fig_centroid is not None and img_feat is not None:
            fig_centroid_arr = np.array(fig_centroid)
            min_d = min(len(img_feat), len(fig_centroid_arr))
            fig_sim = float(cosine_similarity(
                img_feat[:min_d].reshape(1, -1),
                fig_centroid_arr[:min_d].reshape(1, -1)
            )[0, 0])
            fig_score = self._sim_to_score(
                fig_sim, fig_stat.get('sim_mean', 0.5), fig_stat.get('sim_std', 0.15),
                fig_stat.get('sim_min', 0.1), fig_stat.get('sim_max', 0.9),
            )
        else:
            fig_score = 60.0
            fig_sim = 0.0

        o_avg_imgs = fig_stat.get('avg_image_count', 20)
        n_imgs = len(images)
        fig_captions = structure.get('figure_caption_count', 0)
        table_count = structure.get('table_count', 0)

        fig_diff = []
        if n_imgs < o_avg_imgs * 0.4:
            fig_diff.append(f"å›¾è¡¨æ•°é‡åå°‘ ({n_imgs}å¼  vs Oå¥–å¹³å‡{o_avg_imgs:.0f}å¼ )ï¼Œåº”å¢åŠ å¯è§†åŒ–")
            fig_score *= 0.85
        elif n_imgs > o_avg_imgs * 1.5:
            fig_diff.append(f"å›¾è¡¨æ•°é‡å……è¶³ ({n_imgs}å¼  vs Oå¥–å¹³å‡{o_avg_imgs:.0f}å¼ )")
        else:
            fig_diff.append(f"å›¾è¡¨æ•°é‡åˆç† ({n_imgs}å¼ , Oå¥–å¹³å‡{o_avg_imgs:.0f}å¼ )")

        if fig_captions < 5:
            fig_diff.append(f"å›¾è¡¨æ ‡é¢˜æ•°ä»…{fig_captions}ä¸ªï¼ŒOå¥–é€šå¸¸æœ‰10+ä¸ªæ¸…æ™°çš„å›¾è¡¨æ ‡é¢˜")
        if table_count == 0:
            fig_diff.append("æœªæ£€æµ‹åˆ°è¡¨æ ¼ï¼ŒOå¥–è®ºæ–‡é€šå¸¸æœ‰æ•°æ®å¯¹æ¯”è¡¨æ ¼")
        
        # æ–°å¢ï¼šå›¾ç‰‡ç”»é£/è´¨é‡åˆ†æ
        img_stats = image_result.get('statistical_features', np.zeros(18))
        if len(img_stats) >= 18 and n_imgs > 0:
            chart_ratio = img_stats[6]
            professional = img_stats[13]
            high_res_ratio = img_stats[17]
            color_ratio = img_stats[9]
            
            if chart_ratio < 0.3:
                fig_diff.append(f"å›¾è¡¨ç±»å›¾ç‰‡æ¯”ä¾‹åä½({chart_ratio:.0%})ï¼Œç…§ç‰‡/è£…é¥°å›¾è¾ƒå¤šï¼Œå»ºè®®å¢åŠ ä¸“ä¸šå›¾è¡¨")
                fig_score *= 0.92
            elif chart_ratio > 0.6:
                fig_diff.append(f"å›¾è¡¨ç±»å›¾ç‰‡å æ¯”{chart_ratio:.0%}ï¼Œå›¾è¡¨ç”»é£ä¸“ä¸š")
            
            if professional < 0.3:
                fig_diff.append("å›¾ç‰‡æ•´ä½“ä¸“ä¸šåº¦åä½ï¼Œå»ºè®®ä½¿ç”¨çŸ¢é‡å›¾/é«˜å¯¹æ¯”åº¦é…è‰²")
                fig_score *= 0.90
            elif professional > 0.6:
                fig_diff.append("å›¾ç‰‡ä¸“ä¸šåº¦è¾ƒé«˜ï¼ˆå¯¹æ¯”åº¦ã€æ’ç‰ˆä¸€è‡´æ€§å¥½ï¼‰")
            
            if high_res_ratio < 0.5:
                fig_diff.append(f"é«˜åˆ†è¾¨ç‡å›¾ç‰‡ä»…å {high_res_ratio:.0%}ï¼Œå»ºè®®æå‡å›¾è¡¨æ¸…æ™°åº¦")
            
            if color_ratio > 0.7:
                fig_diff.append("å¤§éƒ¨åˆ†ä¸ºå½©è‰²å›¾è¡¨ï¼Œè§†è§‰æ•ˆæœå¥½")

        fig_score = float(np.clip(fig_score, 0, 100))
        scores['figures'] = round(fig_score, 1)
        details['figures'] = {
            'score': round(fig_score, 1),
            'similarity': round(fig_sim, 4),
            'differences': fig_diff,
        }

        # ---------- 3. å»ºæ¨¡æ·±åº¦ç»´åº¦ ----------
        mod_stat = aspect_stats.get('modeling', {})
        mod_centroid = mod_stat.get('centroid')
        struct_feat = text_result.get('structural_features')

        if mod_centroid is not None and struct_feat is not None:
            mod_centroid_arr = np.array(mod_centroid)
            min_d = min(len(struct_feat), len(mod_centroid_arr))
            mod_sim = float(cosine_similarity(
                struct_feat[:min_d].reshape(1, -1),
                mod_centroid_arr[:min_d].reshape(1, -1)
            )[0, 0])
            mod_score = self._sim_to_score(
                mod_sim, mod_stat.get('sim_mean', 0.5), mod_stat.get('sim_std', 0.15),
                mod_stat.get('sim_min', 0.1), mod_stat.get('sim_max', 0.9),
            )
        else:
            mod_score = 60.0
            mod_sim = 0.0

        o_avg_formula = mod_stat.get('avg_formula_count', 20)
        o_avg_cite = mod_stat.get('avg_citation_count', 30)
        o_avg_complete = mod_stat.get('avg_completeness', 0.8)
        o_avg_adv = mod_stat.get('avg_advanced_sections', 2)
        o_avg_pages = mod_stat.get('avg_page_count', 22)

        formula_count = structure.get('formula_count', 0)
        citation_count = structure.get('citation_count', 0)
        completeness = structure.get('structure_completeness', 0)
        adv_count = structure.get('advanced_section_count', 0)

        mod_diff = []
        if formula_count < o_avg_formula * 0.5:
            mod_diff.append(f"å…¬å¼æ•°é‡åå°‘ ({formula_count}ä¸ª vs Oå¥–å¹³å‡{o_avg_formula:.0f}ä¸ª)ï¼Œå»ºæ¨¡æ·±åº¦ä¸è¶³")
            mod_score *= 0.9
        else:
            mod_diff.append(f"å…¬å¼æ•°é‡{'å……è¶³' if formula_count>=o_avg_formula else 'åˆç†'} ({formula_count}ä¸ª, Oå¥–å¹³å‡{o_avg_formula:.0f}ä¸ª)")

        if citation_count < o_avg_cite * 0.4:
            mod_diff.append(f"å¼•ç”¨åå°‘ ({citation_count}å¤„ vs Oå¥–å¹³å‡{o_avg_cite:.0f}å¤„)ï¼Œæ–‡çŒ®ç»¼è¿°å¯åŠ å¼º")
        if completeness < o_avg_complete - 0.15:
            mod_diff.append(f"ç»“æ„å®Œæ•´åº¦åä½ ({completeness:.0%} vs Oå¥–å¹³å‡{o_avg_complete:.0%})")

        has_sens = structure.get('has_sensitivity_analysis', False)
        has_val = structure.get('has_model_validation', False)
        if not has_sens:
            mod_diff.append("ç¼ºå°‘çµæ•åº¦åˆ†æï¼ŒOå¥–è®ºæ–‡é€šå¸¸åŒ…å« Sensitivity Analysis")
        if not has_val:
            mod_diff.append("ç¼ºå°‘æ¨¡å‹éªŒè¯ï¼Œå»ºè®®åŠ å…¥ Model Validation/Testing éƒ¨åˆ†")
        if adv_count < o_avg_adv * 0.5:
            mod_diff.append(f"é«˜çº§å»ºæ¨¡å†…å®¹åå°‘ ({adv_count}èŠ‚ vs Oå¥–å¹³å‡{o_avg_adv:.0f}èŠ‚)")

        if page_count < o_avg_pages * 0.6:
            mod_diff.append(f"è®ºæ–‡é¡µæ•°åå°‘ ({page_count}é¡µ vs Oå¥–å¹³å‡{o_avg_pages:.0f}é¡µ)")

        mod_score = float(np.clip(mod_score, 0, 100))
        scores['modeling'] = round(mod_score, 1)
        details['modeling'] = {
            'score': round(mod_score, 1),
            'similarity': round(mod_sim, 4),
            'differences': mod_diff,
        }

        return scores, details
    
    @staticmethod
    def _sim_to_score(sim, mean, std, smin, smax):
        """ç›¸ä¼¼åº¦ â†’ 0-100 åˆ†"""
        std = max(std, 0.01)
        if sim >= smax:
            return 100.0
        elif sim >= mean:
            return 85 + 15 * (sim - mean) / max(smax - mean, 1e-6)
        elif sim >= mean - 2 * std:
            return 50 + 35 * (sim - (mean - 2 * std)) / max(2 * std, 1e-6)
        else:
            thr = mean - 2 * std
            return max(0, 50 * (sim - smin) / max(thr - smin, 1e-6))
    
    def _compute_score(self, similarity: float, stats: dict) -> float:
        """å°†ä½™å¼¦ç›¸ä¼¼åº¦æ˜ å°„ä¸º 0-100 åˆ†æ•°"""
        sim_mean = stats['similarity_mean']
        sim_std = stats['similarity_std']
        sim_min = stats['similarity_min']
        sim_max = stats['similarity_max']
        
        if similarity >= sim_max:
            score = 100.0
        elif similarity >= sim_mean:
            score = 85 + 15 * (similarity - sim_mean) / max(sim_max - sim_mean, 1e-6)
        elif similarity >= sim_mean - 2 * sim_std:
            score = 50 + 35 * (similarity - (sim_mean - 2 * sim_std)) / max(2 * sim_std, 1e-6)
        else:
            threshold = sim_mean - 2 * sim_std
            if similarity > sim_min:
                score = 50 * (similarity - sim_min) / max(threshold - sim_min, 1e-6)
            else:
                score = max(0.0, 50 * (similarity - sim_min) / max(threshold - sim_min, 1e-6))
        
        return float(np.clip(score, 0, 100))
    
    def print_result(self, result: dict):
        """æ ¼å¼åŒ–æ‰“å°é¢„æµ‹ç»“æœï¼ˆå«ä¸‰ç»´åº¦å­åˆ†ï¼‰"""
        if not result.get('success', False):
            logger.info(f"\nâŒ é¢„æµ‹å¤±è´¥: {result.get('error', 'Unknown')}")
            return
        
        probs = result['probabilities']
        problem = result['problem']
        contest = result['contest']
        year = result['year']
        score = result['score']
        similarity = result['similarity']
        quality_tier = result['quality_tier']
        emoji_tier = result['emoji']
        aspect_scores = result.get('aspect_scores', {})
        aspect_details = result.get('aspect_details', {})
        
        logger.info(f"\n{'='*64}")
        logger.info(f"  MCM/ICM è·å¥–æ¦‚ç‡é¢„æµ‹ç»“æœ (v2)")
        logger.info(f"{'='*64}")
        
        logger.info(f"\n  é¢˜ç›®: {contest} Problem {problem} ({year})")
        profile = get_problem_profile(problem)
        logger.info(f"  ç±»å‹: {profile['name']}")
        
        # å…ˆéªŒä¿¡æ¯
        prior = get_award_prior(problem, year)
        logger.info(f"\n  ğŸ“Š è¯¥é¢˜å†å²è·å¥–æ¯”ä¾‹:")
        logger.info(f"     O: {prior['O']*100:.2f}% | F: {prior['F']*100:.2f}% | "
                     f"M: {prior['M']*100:.1f}% | H: {prior['H']*100:.1f}% | S: {prior['S']*100:.1f}%")
        
        logger.info(f"\n  {emoji_tier} ç»¼åˆè¯„åˆ†: {score:.1f} / 100 ({quality_tier})")
        logger.info(f"  ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}")
        
        stats = self.model_data['stats']
        logger.info(f"  Oå¥–å‚è€ƒ: å‡å€¼={stats['similarity_mean']:.4f}, "
                     f"èŒƒå›´=[{stats['similarity_min']:.4f}, {stats['similarity_max']:.4f}]")
        
        # ==================== ä¸‰ç»´åº¦å­åˆ† ====================
        logger.info(f"\n{'â”€'*64}")
        logger.info(f"  ğŸ“ ä¸‰ç»´åº¦å¯¹æ¯”è¯„åˆ† (ä¸ O å¥–è®ºæ–‡å¯¹æ¯”)")
        logger.info(f"{'â”€'*64}")
        
        aspect_names = {
            'abstract': ('ğŸ“ æ‘˜è¦å†™ä½œ', 'æ‘˜è¦è¯­ä¹‰ä¸Oå¥–çš„æ¥è¿‘ç¨‹åº¦ã€å†™ä½œè´¨é‡'),
            'figures':  ('ğŸ“Š å›¾è¡¨æ°´å¹³', 'å›¾è¡¨ç‰¹å¾ä¸Oå¥–çš„æ¥è¿‘ç¨‹åº¦ã€æ•°é‡è´¨é‡'),
            'modeling': ('ğŸ”¬ å»ºæ¨¡æ·±åº¦', 'ç»“æ„ç‰¹å¾ä¸Oå¥–çš„æ¥è¿‘ç¨‹åº¦ã€å…¬å¼å¼•ç”¨ç­‰'),
        }
        
        for key in ['abstract', 'figures', 'modeling']:
            name, desc = aspect_names[key]
            s = aspect_scores.get(key, 0)
            detail = aspect_details.get(key, {})
            diffs = detail.get('differences', [])
            sim_val = detail.get('similarity', 0)
            
            # åˆ†æ•°æ¡
            bar_full = 20
            bar_filled = int(s / 100 * bar_full)
            bar = "â–ˆ" * bar_filled + "â–‘" * (bar_full - bar_filled)
            
            logger.info(f"\n  {name}:  {s:.0f} / 100")
            logger.info(f"    [{bar}] (ç›¸ä¼¼åº¦={sim_val:.4f})")
            
            for d in diffs:
                logger.info(f"    â€¢ {d}")
        
        # ==================== æ¦‚ç‡åˆ†å¸ƒ ====================
        logger.info(f"\n{'â”€'*64}")
        logger.info(f"  ğŸ¯ é¢„æµ‹è·å¥–æ¦‚ç‡:")
        prob_str = EnhancedAwardEstimator.format_probabilities(probs, problem)
        logger.info(prob_str)
        
        # æè¿°
        logger.info(f"\n  {result['description']}")
        
        # å…ƒæ•°æ®
        meta = result['metadata']
        structure = meta.get('structure', {})
        logger.info(f"\n{'â”€'*64}")
        logger.info(f"  ğŸ“‹ è®ºæ–‡åŸºæœ¬ä¿¡æ¯:")
        logger.info(f"     æ‘˜è¦: {meta['abstract_length']} å­—ç¬¦")
        logger.info(f"     å…¨æ–‡: ~{meta['full_text_length']//1000}k å­—ç¬¦")
        logger.info(f"     å›¾è¡¨: {meta['image_count']} å¼ å›¾ç‰‡ + "
                     f"{structure.get('figure_caption_count', 0)} ä¸ªå›¾è¡¨æ ‡é¢˜ + "
                     f"{structure.get('table_count', 0)} ä¸ªè¡¨æ ¼")
        logger.info(f"     å…¬å¼: {structure.get('formula_count', 0)} ä¸ª")
        logger.info(f"     å¼•ç”¨: {structure.get('citation_count', 0)} å¤„å¼•ç”¨, "
                     f"{meta['ref_count']} æ¡å‚è€ƒæ–‡çŒ®")
        logger.info(f"     é¡µæ•°: {meta['page_count']} é¡µ")
        
        # ç»“æ„å®Œæ•´åº¦
        completeness = structure.get('structure_completeness', 0)
        logger.info(f"\n  ğŸ“ ç»“æ„å®Œæ•´åº¦: {completeness:.0%}")
        section_flags = [
            ('æ‘˜è¦', structure.get('has_abstract', False)),
            ('å¼•è¨€', structure.get('has_introduction', False)),
            ('æ–¹æ³•', structure.get('has_methodology', False)),
            ('ç»“æœ', structure.get('has_results', False)),
            ('ç»“è®º', structure.get('has_conclusion', False)),
            ('å‚è€ƒæ–‡çŒ®', structure.get('has_references', False)),
        ]
        present = [name for name, flag in section_flags if flag]
        missing = [name for name, flag in section_flags if not flag]
        logger.info(f"     âœ“ åŒ…å«: {', '.join(present) if present else 'æ— '}")
        if missing:
            logger.info(f"     âœ— ç¼ºå°‘: {', '.join(missing)}")
        
        advanced = []
        if structure.get('has_sensitivity_analysis'):
            advanced.append('çµæ•åº¦åˆ†æ')
        if structure.get('has_model_validation'):
            advanced.append('æ¨¡å‹éªŒè¯')
        if structure.get('has_strengths_weaknesses'):
            advanced.append('ä¼˜ç¼ºç‚¹åˆ†æ')
        if structure.get('has_future_work'):
            advanced.append('æœªæ¥å·¥ä½œ')
        
        if advanced:
            logger.info(f"     â­ é«˜çº§å†…å®¹: {', '.join(advanced)}")
        else:
            logger.info(f"     âš ï¸ å»ºè®®æ·»åŠ : çµæ•åº¦åˆ†æ, æ¨¡å‹éªŒè¯, ä¼˜ç¼ºç‚¹åˆ†æ")
        
        logger.info(f"\n{'='*64}")
    
    def batch_predict(self, pdf_dir: str, problem: str = None, 
                      year: int = None) -> list:
        """æ‰¹é‡é¢„æµ‹ç›®å½•ä¸‹çš„æ‰€æœ‰PDF"""
        pdf_dir = Path(pdf_dir)
        
        if not pdf_dir.exists():
            logger.error(f"ç›®å½•ä¸å­˜åœ¨: {pdf_dir}")
            return []
        
        pdf_files = list(pdf_dir.rglob("*.pdf"))
        
        if not pdf_files:
            logger.error(f"ç›®å½•ä¸­æ²¡æœ‰PDFæ–‡ä»¶")
            return []
        
        logger.info(f"\næ‰¹é‡é¢„æµ‹: {len(pdf_files)} ä¸ªæ–‡ä»¶")
        logger.info("=" * 60)
        
        results = []
        for i, pdf_file in enumerate(pdf_files, 1):
            logger.info(f"\n[{i}/{len(pdf_files)}] {pdf_file.name}")
            result = self.predict(str(pdf_file), problem, year, verbose=False)
            
            if result.get('success'):
                results.append(result)
                probs = result['probabilities']
                best = max(probs, key=probs.get)
                logger.info(f"  {result['emoji']} Score={result['score']:.1f} | "
                             f"Problem {result['problem']} | "
                             f"Best={best}({probs[best]*100:.1f}%)")
            else:
                logger.info(f"  âŒ {result.get('error', 'Failed')}")
        
        # æ±‡æ€»ç»Ÿè®¡
        if results:
            self._print_batch_summary(results)
        
        return results
    
    def _print_batch_summary(self, results: list):
        """æ‰“å°æ‰¹é‡é¢„æµ‹æ±‡æ€»"""
        logger.info(f"\n{'='*60}")
        logger.info(f"  æ‰¹é‡é¢„æµ‹æ±‡æ€» ({len(results)} ç¯‡)")
        logger.info(f"{'='*60}")
        
        scores = [r['score'] for r in results]
        logger.info(f"\n  è¯„åˆ†ç»Ÿè®¡:")
        logger.info(f"    å¹³å‡åˆ†: {np.mean(scores):.1f}")
        logger.info(f"    ä¸­ä½æ•°: {np.median(scores):.1f}")
        logger.info(f"    æœ€é«˜åˆ†: {np.max(scores):.1f}")
        logger.info(f"    æœ€ä½åˆ†: {np.min(scores):.1f}")
        
        # å„å¥–é¡¹å¹³å‡æ¦‚ç‡
        avg_probs = {'O': 0, 'F': 0, 'M': 0, 'H': 0, 'S': 0}
        for r in results:
            for award, prob in r['probabilities'].items():
                avg_probs[award] += prob
        avg_probs = {k: v / len(results) for k, v in avg_probs.items()}
        
        logger.info(f"\n  å¹³å‡è·å¥–æ¦‚ç‡:")
        for award in ['O', 'F', 'M', 'H', 'S']:
            prob = avg_probs[award]
            bar = "â–ˆ" * int(prob * 40)
            logger.info(f"    {award}: {prob*100:5.1f}% {bar}")
        
        # æœ€å¯èƒ½è·å¥–åˆ†å¸ƒ
        best_awards = [max(r['probabilities'], key=r['probabilities'].get) for r in results]
        from collections import Counter
        award_counts = Counter(best_awards)
        
        logger.info(f"\n  æœ€å¯èƒ½è·å¥–åˆ†å¸ƒ:")
        for award in ['O', 'F', 'M', 'H', 'S']:
            count = award_counts.get(award, 0)
            pct = count / len(results) * 100
            logger.info(f"    {award}: {count} ç¯‡ ({pct:.0f}%)")
        
        logger.info(f"\n{'='*60}")


def main():
    parser = argparse.ArgumentParser(
        description="MCM/ICM è®ºæ–‡è·å¥–æ¦‚ç‡é¢„æµ‹ v2",
        epilog="ç¤ºä¾‹:\n"
               "  python predict_award.py paper.pdf\n"
               "  python predict_award.py paper.pdf --problem A --year 2024\n"
               "  python predict_award.py ./papers/ --batch\n",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument("pdf_path", help="PDFæ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--problem", "-p", type=str, choices=list('ABCDEF'),
                        help="é¢˜ç›® (A-F)ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨æ£€æµ‹")
    parser.add_argument("--year", "-y", type=int, 
                        help="å¹´ä»½ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨æ£€æµ‹")
    parser.add_argument("--model", "-m", default="models/scoring_model.pkl",
                        help="æ¨¡å‹è·¯å¾„ (é»˜è®¤: models/scoring_model.pkl)")
    parser.add_argument("--batch", "-b", action="store_true",
                        help="æ‰¹é‡é¢„æµ‹ç›®å½•ä¸‹æ‰€æœ‰PDF")
    parser.add_argument("--quiet", "-q", action="store_true",
                        help="å®‰é™æ¨¡å¼ï¼ˆå‡å°‘è¾“å‡ºï¼‰")
    parser.add_argument("--show-priors", action="store_true",
                        help="æ˜¾ç¤ºå„é¢˜ç›®çš„å†å²è·å¥–æ¯”ä¾‹æ•°æ®")
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºå…ˆéªŒæ•°æ®
    if args.show_priors:
        print(list_available_data())
        return
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    try:
        predictor = AwardPredictor(model_path=args.model)
    except FileNotFoundError as e:
        logger.error(str(e))
        return
    
    # é¢„æµ‹
    if args.batch or os.path.isdir(args.pdf_path):
        predictor.batch_predict(args.pdf_path, args.problem, args.year)
    else:
        predictor.predict(
            args.pdf_path, 
            problem=args.problem, 
            year=args.year,
            verbose=not args.quiet,
        )


if __name__ == "__main__":
    main()
