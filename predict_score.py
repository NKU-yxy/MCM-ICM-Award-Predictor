"""
Oå¥–ç›¸ä¼¼åº¦æ‰“åˆ†é¢„æµ‹è„šæœ¬
è¾“å‡º0-100çš„ç›¸ä¼¼åº¦åˆ†æ•°
"""

import os
import sys
import pickle
import numpy as np
from pathlib import Path
import argparse
import logging
from sklearn.metrics.pairwise import cosine_similarity

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.pdf_parser import extract_paper_content
from src.text_features import extract_text_features
from src.image_features import extract_image_features
from src.feature_fusion import fuse_features

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


def load_scoring_model(model_path: str = "models/scoring_model.pkl"):
    """åŠ è½½æ‰“åˆ†æ¨¡å‹"""
    if not os.path.exists(model_path):
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        logger.error("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬: python scripts/train_scoring_model.py")
        return None
    
    with open(model_path, 'rb') as f:
        model_data = pickle.load(f)
    
    return model_data


def compute_score(features: np.ndarray, centroid: np.ndarray, stats: dict) -> float:
    """
    è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•° (0-100)
    
    åŸºäºä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ˜ å°„åˆ°0-100åŒºé—´
    
    ç­–ç•¥:
    1. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (èŒƒå›´ -1 åˆ° 1)
    2. ä½¿ç”¨Oå¥–ç»Ÿè®¡ä¿¡æ¯æ ‡å®šåˆ†æ•°
    3. similarity_mean å¯¹åº” ~85åˆ†ï¼ˆå¹³å‡Oå¥–æ°´å¹³ï¼‰
    4. similarity_max å¯¹åº” ~100åˆ†ï¼ˆæœ€ä¼˜Oå¥–ï¼‰
    5. similarity_mean - 2*std å¯¹åº” ~50åˆ†ï¼ˆåŠæ ¼çº¿ï¼‰
    """
    # ç»´åº¦å¯¹é½ï¼ˆæ¨¡å‹å¯èƒ½ç”¨æ—§ç‰ˆç‰¹å¾è®­ç»ƒï¼‰
    f_dim = features.shape[-1] if features.ndim > 1 else features.shape[0]
    c_dim = centroid.shape[-1] if centroid.ndim > 1 else centroid.shape[0]
    if f_dim != c_dim:
        min_dim = min(f_dim, c_dim)
        logger.warning(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: features={f_dim}, centroid={c_dim}ï¼Œæˆªæ–­åˆ°{min_dim}ç»´")
        features = features.flatten()[:min_dim]
        centroid = centroid.flatten()[:min_dim]
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = cosine_similarity(
        features.reshape(1, -1),
        centroid.reshape(1, -1)
    )[0, 0]
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    sim_mean = stats['similarity_mean']
    sim_std = stats['similarity_std']
    sim_min = stats['similarity_min']
    sim_max = stats['similarity_max']
    
    # åˆ†æ•°æ˜ å°„ï¼ˆæ·»åŠ é™¤é›¶ä¿æŠ¤ï¼‰
    if similarity >= sim_max:
        score = 100.0
    elif similarity >= sim_mean:
        score = 85 + 15 * (similarity - sim_mean) / max(sim_max - sim_mean, 1e-6)
    elif similarity >= sim_mean - 2 * sim_std:
        score = 50 + 35 * (similarity - (sim_mean - 2*sim_std)) / max(2*sim_std, 1e-6)
    else:
        threshold = sim_mean - 2 * sim_std
        if similarity > sim_min:
            score = 50 * (similarity - sim_min) / max(threshold - sim_min, 1e-6)
        else:
            score = max(0.0, 50 * (similarity - sim_min) / max(threshold - sim_min, 1e-6))
    
    # ç¡®ä¿èŒƒå›´
    score = np.clip(score, 0, 100)
    
    return float(score), float(similarity)


def get_score_interpretation(score: float) -> tuple:
    """
    è§£é‡Šåˆ†æ•°ç­‰çº§
    
    è¿”å›: (ç­‰çº§, æè¿°, é¢œè‰²æ ‡è®°)
    """
    if score >= 95:
        return "å“è¶Š", "è¾¾åˆ°é¡¶å°–Oå¥–æ°´å¹³", "ğŸŒŸ"
    elif score >= 85:
        return "ä¼˜ç§€", "è¾¾åˆ°å…¸å‹Oå¥–æ°´å¹³", "â­"
    elif score >= 75:
        return "è‰¯å¥½", "æ¥è¿‘Oå¥–æ°´å¹³", "âœ¨"
    elif score >= 60:
        return "ä¸­ç­‰", "æœ‰ä¸€å®šæ½œåŠ›ï¼Œéœ€æ”¹è¿›", "ğŸ’¡"
    elif score >= 40:
        return "åŠæ ¼", "åŸºç¡€å¯ç”¨ï¼Œéœ€å¤§å¹…æå‡", "ğŸ“"
    else:
        return "è¾ƒå¼±", "ä¸Oå¥–æ ‡å‡†å·®è·è¾ƒå¤§", "âš ï¸"


def predict_paper(pdf_path: str, model_data: dict, verbose: bool = True) -> dict:
    """
    é¢„æµ‹å•ç¯‡è®ºæ–‡çš„åˆ†æ•°
    
    è¿”å›:
        dict: åŒ…å«åˆ†æ•°ã€ç›¸ä¼¼åº¦ã€ç­‰çº§ç­‰ä¿¡æ¯
    """
    if not os.path.exists(pdf_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        return None
    
    try:
        # 1. æå–å†…å®¹
        if verbose:
            logger.info(f"\nå¤„ç†è®ºæ–‡: {os.path.basename(pdf_path)}")
            logger.info("="*60)
        
        content = extract_paper_content(pdf_path)
        
        if not content['abstract']:
            logger.error("æœªæ‰¾åˆ°æ‘˜è¦ï¼")
            return None
        
        if verbose:
            logger.info(f"âœ“ æå–æ‘˜è¦: {len(content['abstract'])} å­—ç¬¦")
            logger.info(f"âœ“ æå–å›¾ç‰‡: {len(content['images'])} å¼ ")
        
        # 2. æå–ç‰¹å¾
        text_feat = extract_text_features(content['abstract'])
        image_feat = extract_image_features(content['images'])
        
        # ä»è·¯å¾„æå–å…ƒæ•°æ®
        path_parts = Path(pdf_path).parts
        year, contest, problem = None, 'MCM', 'A'
        
        # å°è¯•ä»è·¯å¾„æå–
        for part in path_parts:
            if part.isdigit() and 2010 <= int(part) <= 2030:
                year = int(part)
            if '_' in part and part.count('_') == 1:
                c, p = part.split('_')
                if c in ['MCM', 'ICM'] and p in 'ABCDEF':
                    contest, problem = c, p
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not year:
            year = 2025
        
        # ä»PDFå…ƒæ•°æ®ä¸­è·å–é¡µæ•°å’Œå‚è€ƒæ–‡çŒ®æ•°
        pdf_metadata = content.get('metadata', {})
        page_count = pdf_metadata.get('page_count', 20)
        ref_count = pdf_metadata.get('ref_count', 15)
        
        # èåˆç‰¹å¾ï¼ˆå·²åœ¨fusionå±‚åšper-group L2å½’ä¸€åŒ–ï¼‰
        features = fuse_features(text_feat, image_feat, year, contest, problem,
                                 page_count=page_count, ref_count=ref_count)
        
        # 3. è®¡ç®—åˆ†æ•°
        centroid = model_data['centroid']
        stats = model_data['stats']
        
        score, similarity = compute_score(features, centroid, stats)
        
        # 4. è§£é‡Š
        level, description, emoji = get_score_interpretation(score)
        
        result = {
            'score': score,
            'similarity': similarity,
            'level': level,
            'description': description,
            'emoji': emoji,
            'filename': os.path.basename(pdf_path),
            'abstract_length': len(content['abstract']),
            'image_count': len(content['images'])
        }
        
        if verbose:
            logger.info(f"\n{'='*60}")
            logger.info(f"è¯„åˆ†ç»“æœ")
            logger.info(f"{'='*60}")
            logger.info(f"\n  {emoji} åˆ†æ•°: {score:.1f} / 100")
            logger.info(f"  ç­‰çº§: {level}")
            logger.info(f"  è¯„ä»·: {description}")
            logger.info(f"\n  ç›¸ä¼¼åº¦: {similarity:.4f}")
            logger.info(f"  Oå¥–å¹³å‡: {stats['similarity_mean']:.4f}")
            logger.info(f"\n{'='*60}")
        
        return result
        
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def batch_predict(pdf_dir: str, model_data: dict):
    """æ‰¹é‡é¢„æµ‹ç›®å½•ä¸‹çš„æ‰€æœ‰PDF"""
    pdf_dir = Path(pdf_dir)
    
    if not pdf_dir.exists():
        logger.error(f"ç›®å½•ä¸å­˜åœ¨: {pdf_dir}")
        return
    
    pdf_files = list(pdf_dir.rglob("*.pdf"))
    
    if not pdf_files:
        logger.error(f"ç›®å½•ä¸­æ²¡æœ‰PDFæ–‡ä»¶: {pdf_dir}")
        return
    
    logger.info(f"\næ‰¹é‡é¢„æµ‹: {len(pdf_files)} ä¸ªæ–‡ä»¶")
    logger.info("="*60)
    
    results = []
    
    for pdf_file in pdf_files:
        result = predict_paper(str(pdf_file), model_data, verbose=False)
        
        if result:
            results.append(result)
            logger.info(f"{result['emoji']} {result['score']:5.1f} | {result['filename']}")
    
    # ç»Ÿè®¡
    if results:
        scores = [r['score'] for r in results]
        logger.info(f"\n{'='*60}")
        logger.info("æ‰¹é‡ç»Ÿè®¡")
        logger.info(f"{'='*60}")
        logger.info(f"  æ€»è®¡: {len(scores)} ç¯‡")
        logger.info(f"  å¹³å‡åˆ†: {np.mean(scores):.1f}")
        logger.info(f"  æ ‡å‡†å·®: {np.std(scores):.1f}")
        logger.info(f"  æœ€é«˜åˆ†: {np.max(scores):.1f}")
        logger.info(f"  æœ€ä½åˆ†: {np.min(scores):.1f}")
        logger.info(f"{'='*60}")


def main():
    parser = argparse.ArgumentParser(description="Oå¥–ç›¸ä¼¼åº¦æ‰“åˆ†é¢„æµ‹")
    parser.add_argument("pdf_path", help="PDFæ–‡ä»¶è·¯å¾„æˆ–ç›®å½•")
    parser.add_argument("--model", default="models/scoring_model.pkl", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--batch", action="store_true", help="æ‰¹é‡é¢„æµ‹æ¨¡å¼")
    
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹
    model_data = load_scoring_model(args.model)
    
    if model_data is None:
        return
    
    logger.info(f"\nå·²åŠ è½½æ¨¡å‹ (åŸºäº {model_data['n_papers']} ç¯‡ O å¥–è®ºæ–‡)")
    
    # é¢„æµ‹
    if args.batch or os.path.isdir(args.pdf_path):
        batch_predict(args.pdf_path, model_data)
    else:
        predict_paper(args.pdf_path, model_data)


if __name__ == "__main__":
    main()
